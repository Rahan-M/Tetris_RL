{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f5f5fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, random\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e63946a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tetris'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-1169212605.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproject_root\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtetris\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtetris_env\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTetrisGymEnv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0magents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_learning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_network\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mQNetwork\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0magents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_learning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay_buffer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mReplayBuffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tetris'",
      "",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "\n",
    "# Go two levels up to reach Tetris_RL/\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"../..\"))\n",
    "sys.path.insert(0, project_root)\n",
    "\n",
    "from tetris.tetris_env import TetrisGymEnv\n",
    "from agents.q_learning.q_network import QNetwork\n",
    "from agents.q_learning.replay_buffer import ReplayBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79194746",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_q_learning(\n",
    "    episodes=500,\n",
    "    batch_size=64,\n",
    "    gamma=0.99, # controls how much the agent cares about the long term rewards, we set this high(close to 1) as tetris is a long horizon game\n",
    "    epsilon_start=1.0,\n",
    "    epsilon_end=0.05,\n",
    "    epsilon_decay=0.995,\n",
    "    lr=1e-3, #Controls how fast the Q-network updates during gradient descent. 1e-3 is the standard default (step size of weight updates)\n",
    "    buffer_capacity=50000\n",
    "):\n",
    "    env=TetrisGymEnv(observation_type=\"heuristics\")\n",
    "    input_dim=3\n",
    "    output_dim=env.action_space.n\n",
    "\n",
    "    q_network = QNetwork(input_dim, output_dim)\n",
    "    optimizer = optim.Adam(q_network.parameters(), lr=lr)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    replay_buffer = ReplayBuffer(buffer_capacity)\n",
    "\n",
    "    epsilon = epsilon_start\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        state=env.reset()\n",
    "        episode_reward=0\n",
    "        done=False\n",
    "\n",
    "        while not done:\n",
    "            if random.random()<epsilon:\n",
    "                action=env.action_space.sample()\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    state_tensor=torch.tensor(state, dtype=torch.float32).unsqueeze(0) # (batch_size, features) here (1,3) without unsqueeze batch size will be (3,)\n",
    "                    q_values=q_network(state_tensor) \n",
    "                    # When you call q_network(x)\n",
    "                    # PyTorch automatically calls the forward() function.\n",
    "                    action=torch.argmax(q_values).item() # returns index of largest value, item() converts tensor to integer\n",
    "\n",
    "            # take an action and record new state\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "\n",
    "            # store in replay buffer\n",
    "            replay_buffer.push(state, action, reward, next_state, done)\n",
    "\n",
    "            state=next_state\n",
    "            episode_reward+=reward\n",
    "\n",
    "            if len(replay_buffer) > batch_size:\n",
    "                states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n",
    "                states=torch.tensor(states, dtype=torch.float32)\n",
    "                actions=torch.tensor(actions, dtype=torch.long)\n",
    "                rewards=torch.tensor(rewards, dtype=torch.float32)\n",
    "                next_states=torch.tensor(next_states, dtype=torch.float32)\n",
    "                dones=torch.tensor(dones, dtype=torch.float32)\n",
    "\n",
    "                q_values=q_network(states)\n",
    "                q_s_a=q_values.gather(1, actions.unsqueeze(1)).squeeze(1) # q values of the actions taken are stored here shape is (64,)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    next_q = q_network(next_states)\n",
    "                    max_next_q = next_q.max(1)[0] # max q for each action, max_next_q is shape (batch,)\n",
    "                    target = rewards + gamma * max_next_q * (1 - dones) # target coming from the Bellman equation.\n",
    "\n",
    "                # target is what q network should output ideally\n",
    "\n",
    "                loss = loss_fn(q_s_a, target) # loss = mean((predicted - target)^2)\n",
    "\n",
    "                # back prop\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                epsilon = max(epsilon_end, epsilon * epsilon_decay)\n",
    "\n",
    "        print(f\"Episode {episode+1}/{episodes} | Reward: {episode_reward:.2f} | Epsilon: {epsilon:.3f}\")\n",
    "\n",
    "    print(\"Training complete.\")\n",
    "    torch.save(q_network.state_dict(), \"models/q_learning/model.pth\")\n",
    "    print(\"Model saved to models/q_learning/model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b143cd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_q_learning()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
